{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some dogs\n",
    "\n",
    "Lets start by downloading a bunch of random dog images to use as examples.\n",
    "\n",
    "Below, I've set the number to 100, but you can change it to more or less if you like. This took about 10 seconds on my home network.\n",
    "\n",
    "Each time you run the two cells below it will try to download N new images, ignoring duplicates. So if you run them 3 times, odds are good you end up with about 300 images. All images will be saved to the `example_dog_images` folder. Drag/save any images you want to test with to that folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dog images to try and download\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "folder_path = \"example_dog_images\"\n",
    "\n",
    "def download_image(url, folder):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath): \n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return filepath\n",
    "    return None\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Get N random dog image URLS\n",
    "response = requests.get(f\"https://dog.ceo/api/breeds/image/random/{N}\")\n",
    "urls = response.json()[\"message\"]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(download_image, url, folder_path) for url in urls]\n",
    "    for future in as_completed(futures):\n",
    "        filepath = future.result()\n",
    "        if filepath:\n",
    "            print(f\"Downloaded: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at some of the images that we've downloaded. You can run this cell multiple times to see more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get all image files from the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "\n",
    "# Randomly select 6 images\n",
    "random_images = random.sample(image_files, 6)\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('6 Dogs we downloaded', fontsize=16)\n",
    "\n",
    "# Plot each image\n",
    "for ax, image_file in zip(axes.ravel(), random_images):\n",
    "    img = Image.open(os.path.join(folder_path, image_file))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(image_file, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some models\n",
    "\n",
    "Time for a dash of machine learning ðŸ˜ˆ. Let's download some models. This took a couple of minutes on my home network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# paths and file types\n",
    "image_types = (\".png\", \".jpg\", \".jpeg\", \".gif\")\n",
    "embeddings_cache = \"dogs_embeddings.pkl\"\n",
    "\n",
    "\n",
    "def get_embedding(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "        return outputs.cpu().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_dog_embeddings(folder_path):\n",
    "    embeddings = {}\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path) if f.lower().endswith(image_types)\n",
    "    ]\n",
    "\n",
    "    for image_file in tqdm(image_files, desc=\"Calculating dog embeddings\"):\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        embedding = get_embedding(image_path)\n",
    "        embeddings[image_file] = embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# load model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cude\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "cache_path = os.path.join(folder_path, embeddings_cache)\n",
    "if os.path.exists(cache_path):\n",
    "    with open(cache_path, \"rb\") as f:\n",
    "        dog_embeddings = pickle.load(f)\n",
    "        print(f\"Loaded cached embeddings found at: {cache_path}\")\n",
    "else:\n",
    "    dog_embeddings = calculate_dog_embeddings(folder_path)\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(dog_embeddings, f)\n",
    "    print(f\"Calculated embeddings for {len(dog_embeddings)} images.\")\n",
    "    print(f\"Saved to: {cache_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_doggleganger(selfie_path, dog_embeddings, top_n=3):\n",
    "    selfie_embedding = get_embedding(selfie_path)\n",
    "\n",
    "    similarities = {}\n",
    "    for dog_file, dog_embedding in dog_embeddings.items():\n",
    "        similarity = 1 - cosine(selfie_embedding, dog_embedding)\n",
    "        similarities[dog_file] = similarity\n",
    "\n",
    "    # sort in descending order and get the top_n results\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, finally time to find your DOGGLEGANGER.\n",
    "\n",
    "Upload/save an image by drag-and-dropping it into the sidebar. On my machine I added Lizzy's selfie and called it `lizzie_selfie.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the selfie path here\n",
    "selfie_path = 'lizzie_selfie.png'\n",
    "\n",
    "top_results = find_doggleganger(selfie_path, dog_embeddings)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for idx, (dog_file, similarity) in enumerate(top_results):\n",
    "    dog_filepath = os.path.join(folder_path, dog_file)\n",
    "    img = Image.open(dog_filepath)  # open the image file\n",
    "    plt.subplot(1, len(top_results), idx + 1)  # create a subplot for each image\n",
    "    plt.imshow(img)  # display the image\n",
    "    plt.title(f'{dog_file}\\nSimilarity: {similarity:.2f}')\n",
    "    plt.axis('off')  # hide axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with our example images -- Lizzie and [cute dog name that i'm blanking on]. Ideally, this should have a higher similarity than any of the other images from our dataset.\n",
    "\n",
    "Note: we're going to give ourselves some grace if the similarity is NOT higher. That's because so far, we're just using an off-the-shelf embedding model. We haven't done any training to try and capture \"similar\" images, we're basically just crossing our fingers that the raw images produce similar features. Maybe the background color or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to try and compare two specific images here, just to play around with it, simply paste their paths into the `selfie` and `dog` variables below\n",
    "selfie = 'lizzie_selfie.png'\n",
    "dog = 'lizzie_puppy.png'\n",
    "\n",
    "selfie_embedding = get_embedding(selfie)\n",
    "dog_embedding = get_embedding(dog)\n",
    "\n",
    "similarity = 1 - cosine(selfie_embedding, dog_embedding)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, im in enumerate([selfie, dog]):\n",
    "    img = Image.open(im)  # open the image file\n",
    "    plt.subplot(1, 2, i+1)  # create a subplot for each image\n",
    "    plt.imshow(img)  # display the image\n",
    "    plt.title(f'{im}\\nSimilarity: {similarity:.2f}')\n",
    "    plt.axis('off')  # hide axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It kinda works!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "- **Add training**: Whip up like, 10-50 training examples and make sure that we can train an alignment layer between \"golden\" pairs of selfies and dogs. Essentially, we should be able to make sure that Lizzie/Puppy has a similarity of 0.99. This won't be perfect with only a few dozen examples, but it'll be quick and prove the case.\n",
    "- **Spotcheck PetFinder**: I think the biggest potential risk is that we just don't get back very many dogs, for most \"reasonable\" location/distance pairs, or that the pictures we get back from PetFinder are pretty bad. Basically, what if you live in the suburbs and set your location to like 5 miles, and there's only 8 dogs up for adoption near you and their pictures are all bad. I'll whip up a notebook to hit that API and just let us play with it directly so we can validate if we even get a reasonable number of cute dogs back from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
